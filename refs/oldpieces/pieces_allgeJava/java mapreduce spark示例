独立模式：scala的sc创建
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)




交互式运行-官方示例程序
./bin/spark-submit examples/src/main/python/pi.py

计数helloworld
lines = sc.textFile("README.md") # Create an RDD called lines
lines.count() # Count the number of items in this RDD
lines.first() # First item in this RDD, i.e. first line of README.md

选出带有“python”的数据行
lines = sc.textFile("README.md")
pythonLines = lines.filter(lambda line: "Python" in line)
pythonLines.first()

独立运行spark而不是交互式
from pyspark import SparkConf, SparkContext
conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)
...

py快速创建RDD
lines = sc.parallelize([“pandas”, “i like pandas”])

RDD上的union操作（更好的办法是过滤一次）
errorsRDD = inputRDD.filter(lamda x: “error” in x)
warningsRDD = inputRDD.filter(lamda x: “warning” in x)
badLinesRDD = errorsRDD.union(warningsRDD)

RDD上的动作实例
print “Input had” + badLinesRDD.count() + “ concerning lines”
print “Here are 10 examples:”
for line in badLinesRDD.take(10):
     print line

map操作实例：
nums = sc.parallelize([1, 2, 3, 4])
squared = nums.map(lambda x: x * x).collect()
for num in squared:
    print "%i " % (num)

flatMap操作：
lines = sc.parallelize(["hello world", "hi"])
words = lines.flatMap(lambda line: line.split(" "))
words.first() # returns "hello"

reduce操作用来求和
sum = rdd.reduce(lambda x, y: x + y)

求平均数map后再reduce
sumCount = nums.aggregate((0, 0),
(lambda acc, value: (acc[0] + value, acc[1] + 1),
(lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))))
return sumCount[0] / float(sumCount[1])


val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)
println(result.count())
println(result.collect().mkString(","))

创建键值对
pairs = lines.map(lambda x: (x.split(" ")[0], x))

筛选
result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)





























